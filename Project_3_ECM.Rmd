---
title: "Project 3"
author: "Evan McLaughlin"
date: "3/25/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(openintro)
library(ggplot2)
library(dplyr)
library(knitr)
library(tinytex)
library(DBI)
library(magrittr)
library(stringr)
library(fmsb)
library(NLP)
library(wordcloud)
library(RColorBrewer)
library(tidytext)
library(tm)
library(ggpubr)
library(stringr)
```

## OVerview

## NLP Analyzing Job Description
```{r}
# first, I'll read in and clean up the job descriptions
jobs_desc_file = "https://raw.githubusercontent.com/evanmclaughlin/ECM607/master/Project1_Description-Data.csv"
jobs_desc = readLines( jobs_desc_file , warn = FALSE)
jobs_desc <- data.frame(do.call('rbind', strsplit(as.character(jobs_desc),'"',fixed=TRUE)))
jobs_desc <- jobs_desc[-1,-1]
jobs_df <- jobs_desc$X2

# It's easier to manipulate this data how we want to by converting it to a tibble
jobs_tbl <- tibble(txt = jobs_df)
#jobs_tbl

#next, let's tokenize the text of the description and execute a word count to get an idea of the most prevalent words. We'll also run the result against a stop words list to exclude words that don't add any value to our analysis such as "the", "and", "that", etc.
token <- jobs_tbl %>%
  unnest_tokens(word, 1) %>%
  anti_join(stop_words)

token_count <- token %>%
  count(word) %>%
  arrange(desc(n))

token_count
```


```{r}
# It's probably also useful to take a look at the most common word pairs, given many of these words are more descriptive in combination with others
token_pairs <- jobs_tbl %>%
  unnest_tokens(pairs, 1, token = "ngrams", n = 2)
token_pairs %>%
  count(pairs) %>%
  arrange(desc(n))
# Now, let's run the pairs against the stop_word database by separating the pairs and eliminating cases where either word appears in the stop_word list
pairs_separated <- token_pairs %>%
  separate(pairs, c("word1", "word2"), sep = " ")

pairs_df <- pairs_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

pairs_count <- pairs_df %>% 
  count(word1, word2, sort = TRUE)

pairs_count
```
```{r}
# Before reuniting these columns, let's quickly go through the prominent words and eliminate more terms that don't add much value by augmenting the stop_words list and running another. We can add to this list if we happen to see any additional words that aren't helpful. 

new_stop <- data.frame(word = c("apply", "london", "remote","remotely", "interview", "salary", "contract", "candidate", "scientist", "scientists", "team", "analyst", "engineer", "engineers", "manager", "managers", "senior", "employment", "experienced", "consultant", "junior", "month", "level", "masters", "rosie", "months", "experience", "level", "orientation", "opportunity", "principal", "benefits", "nick", "days", "day", "role", "francesca", "goldman", "luke", "anna", "date", "charlotte"), lexicon = "custom")
my_stopwords <- rbind(new_stop, stop_words)

pairs_df <- pairs_separated %>%
  filter(!word1 %in% my_stopwords$word) %>%
  filter(!word2 %in% my_stopwords$word)

# Let's now reunite the columns into a single pairs for analysis.

pairs_united <- pairs_df %>%
  unite(term, word1, word2, sep = " ")
df_terms <- pairs_united$term
terms_tbl <- tibble(txt = df_terms)

united_count <- pairs_united %>% 
  count(term, sort = TRUE)

united_count
```


```{r}
library(wordcloud)

terms_tbl %>%
  count(txt) %>%
  with(wordcloud(txt, n, max.words = 100))

library(reshape2)

terms_tbl %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0, colors = c("#F8766D", "#00BFC4")) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100)
```

